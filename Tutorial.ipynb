{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569376bdcfa95d59",
   "metadata": {},
   "source": [
    "# Welcome to this tutorial on image processing!\n",
    "\n",
    "This tutorial will help you get started with your webcam or camera through a very simple but very powerful example: color detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba94def9d8e2fa36",
   "metadata": {},
   "source": [
    "## Introduction to images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf15aa363d8b7c8",
   "metadata": {},
   "source": [
    "**Run the following cell to start!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bef3a75fc94f34",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2; import matplotlib.pyplot as plt\n",
    "cap = cv2.VideoCapture(0); title = \"A picture of the happiest day of my engineering studies\"; _ = cv2.waitKey(500)\n",
    "plt.imshow(cv2.cvtColor(cap.read()[1], cv2.COLOR_BGR2RGB)); plt.axis('off'); plt.title(title); plt.show(); cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce9f579246973a",
   "metadata": {},
   "source": [
    "A lot of things happened to get this picture, so many things that multiple engineering branches are needed to explain it:\n",
    "1. *PHOTONICS*: Photons bounced of you and your environment, travelled through the air and got picked up by the camera sensor.\n",
    "2. *ELECTRONICS*: The camera sensor transformed all the photons in  an array of electrical charges. Those were then transformed from an analog to a digital representation.\n",
    "4. *INFORMATICS*: The digital representation was then stored in the memory of your computer. It could then be manipulated, analyzed or simply displayed, this is what we call image processing!\n",
    "\n",
    "In this tutorial, we will focus on the last part: **how can we process images to measure, analyze, react to our environment**? If you read something that sounded interesting before the image processing step, feel free to ask any questions... All the steps are fascinating!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510afdc3c27df9e5",
   "metadata": {},
   "source": [
    "## Overview of the objectives of this Notebook:\n",
    "\n",
    "1. OpenCV\n",
    "    - Use OpenCV to load and manipulate images\n",
    "    - Setup a video capture with OpenCV\n",
    "\n",
    "2. Colors\n",
    "    - How does a computer perceive color?\n",
    "    - Detect colors in an image\n",
    "    - Color ok, so what?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a376ce07b81cb64",
   "metadata": {},
   "source": [
    "## An engineer never starts from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc488aa7bace003",
   "metadata": {},
   "source": [
    "It would be a massive time loss if we had to manually write scripts to define advanced mathematical operations, define the exact color of every pixel of our screen to display an image, or identify usable USB connections that give us video input. All these issues have been solved and optimized by very talented engineers before us!\n",
    "\n",
    "Let's start with the following libraries that we will use throughout this notebook.\n",
    "- **Numpy** allows us to easily apply mathematical operations\n",
    "- **Matplotlib** allows us to display images\n",
    "- **OpenCV** allows us to fetch images from a camera, but also much more!\n",
    "\n",
    "Run the following cell to import the libraries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5d6c1c5d4f7d6f",
   "metadata": {},
   "source": [
    "Additionally, the assistants are also there to make your life a little bit more easy. Here is a helper function that will allow us to see the colors that we detect later in this notebook. The exact code is not important for the moment, but feel free to come back to it once you feel familiar with OpenCV.\n",
    "\n",
    "Run the following cell to define the function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3c004baa754514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_contours(frame, mask, color):\n",
    "\n",
    "    # BGR to RGB\n",
    "    color = list(reversed(color))\n",
    "    frame = frame.copy()\n",
    "\n",
    "    # Morphological operations\n",
    "    # You will get a chance to dive deeper into this later!\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    mask = cv2.erode(mask, kernel, iterations=2)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "\n",
    "    # Find contours in the masked image, these are the outlines of connected regions in the image\n",
    "    cnts, _ = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If we find any contours\n",
    "    if len(cnts) > 0:\n",
    "        # Sort the contours using area\n",
    "        cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "        # Find the largest contour\n",
    "        cnt = cnts[0]\n",
    "\n",
    "        # Calculate the center of the largest contour\n",
    "        M = cv2.moments(cnt)\n",
    "        center_point = (int(M['m10'] / M['m00']), int(M['m01'] / M['m00']))\n",
    "\n",
    "        # Display text on the frame\n",
    "        cv2.putText(frame, str(center_point), center_point, cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
    "        # Draw contours on the frame\n",
    "        cv2.drawContours(frame, [cnt], 0, color, 3)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3109e3c1a7aa531",
   "metadata": {},
   "source": [
    "## A video is a series of frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802d8fa46536237f",
   "metadata": {},
   "source": [
    "The first step of every image processing pipeline is to get at least one image. OpenCV allows us to easily load images through the `cv2.imread()` function, or to set up camera stream using `cv2.VideoCapture()`. Since the first option is trivial, let's check out the camera stream!\n",
    "\n",
    "Run the following cell to set up a video capture using your webcam. `index=0` should be your webcam, feel free to check if other cameras are available by changing the value of the *index* parameter. OpenCV will tell you if the index is incorrect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8aed4070ffa81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d957cb34aac6f9",
   "metadata": {},
   "source": [
    "Now that we found a working webcam, we can check the expected quality of the frames we will receive from it.\n",
    "\n",
    "Run the following cell to check the resolution and frames per second of your webcam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297cfded891ef8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_width  = cap.get(3)\n",
    "cap_height = cap.get(4)\n",
    "cap_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print(f\"Resolution: {cap_width} x {cap_height}\")\n",
    "print(f\"Frames per second: {cap_fps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643bca05062904a",
   "metadata": {},
   "source": [
    "As you may know, a video is nothing else than a series of still frames that are shown just fast enough to trick your eyes and brain. When you watch a movie, you often only get to see 24 frames per second, and that is already enough!\n",
    "\n",
    "So, let's start with just one frame.\n",
    "\n",
    "*Note: If your camera seems to be obstructed and that you are using a MacBook, check your IPhone!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f808b0a2a8836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a frame\n",
    "ok, frame = cap.read()\n",
    "\n",
    "# quick check to make sure we got a frame\n",
    "if ok:\n",
    "    # let's check what the frame actually is\n",
    "    frame_type = type(frame)\n",
    "    print(f\"1. Our frame is of the following type: {frame_type}\")\n",
    "\n",
    "    # let's check if the frame has the expected size\n",
    "    frame_shape = frame.shape\n",
    "    print(f\"2. Our frame has the following shape: {frame_shape}\")\n",
    "\n",
    "    # let's see this frame\n",
    "    plt.figure()\n",
    "    frame_plt = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # this will be explained later!\n",
    "    plt.imshow(frame_plt)\n",
    "    plt.axis('off') # remove the horizontal and vertical axes\n",
    "    plt.title(\"3. Our frame looks like this:\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fb45b287fd13ba",
   "metadata": {},
   "source": [
    "Let's check the results\n",
    "\n",
    "1. The type of the frame that we got is `<class 'numpy.ndarray'>`. This means that we can easily manipulate our frame using our knowledge of Numpy. Can you correct the first line of the next cell `top_half = ...` to display only the top half of the frame? Can you correct the second line of the next cell `flipped = ...` to display the flipped version of the frame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6a0d41eb08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_half = ...     # Hint: list slicing\n",
    "flipped = ...      # Hint: check around which axis you want to flip the image\n",
    "\n",
    "\n",
    "assert type(top_half) == type(frame), \"top_half is not a numpy array\"\n",
    "assert type(flipped) == type(frame), \"flipped is not a numpy array\"\n",
    "\n",
    "plt.figure()\n",
    "top_half_plt = cv2.cvtColor(top_half, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(top_half_plt)\n",
    "plt.axis('off') # remove the horizontal and vertical axes\n",
    "plt.title(\"Top half of our frame\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "flipped_plt = cv2.cvtColor(flipped, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(flipped_plt)\n",
    "plt.axis('off') # remove the horizontal and vertical axes\n",
    "plt.title(\"Flipped frame\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c5104907ea255",
   "metadata": {},
   "source": [
    "2. Our frame has the expected resolution, but it is stored as `(height, width, 3)`. Why is it a 3D array? What does the third dimension correspond to and why is it size 3?\n",
    "3. Since our frame is actually a numpy array, we can use `matplotlib.pyplot` to display the image. But we did something weird before plotting it with `plt.imshow()`: `frame_plt = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)`. Do you have an idea what this means? Let's come back to it later!\n",
    "\n",
    "First, as promised, a video is a series of frames. Run the following cell to check the real-time webcam feed! Think about the structure of this code, does it make sense to use a *while* loop?\n",
    "\n",
    "*Note 1: Press the **escape** key to end the real-time feed and close the video capture.*\n",
    "\n",
    "*Note 2: If the window does not close, just ignore it for the moment. This is an issue caused by Jupyter Notebooks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b37108ce6fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\n",
    "    ok, frame = cap.read()\n",
    "    if ok:\n",
    "        # cv2.imshow is more efficient than plt.imshow for video content\n",
    "        cv2.imshow(\"Real-time webcam\", frame)\n",
    "\n",
    "    # cv2.waitKey is used to wait to check keyboard or mouse inputs, 1 means 1 millisecond\n",
    "    # The number 27 corresponds to the escape key\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "# Destroy all OpenCV windows (can fail in Jupyter Notebook environment)\n",
    "cv2.destroyAllWindows()\n",
    "# Close the video capture, deactivating the camera\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d31881869820b9",
   "metadata": {},
   "source": [
    "## Processing color in images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d201d7ad0b4155",
   "metadata": {},
   "source": [
    "First of all, we need to understand how a computer perceives color. The answer is that it is surprisingly similar to humans... and that is not a coincidence! Digital colors are defined based on how our eyes interpret colors.\n",
    "\n",
    "You may remember that the center of your retina is covered in little conical photoreceptors which we call *cones*. There are three kinds of cones that are sensitive to different wavelength (long, medium and short), which roughly correspond to red, green and blue light. All the colors of the rainbow are actually not directly seen by our eyes, but, as is often the case, created by our brain.\n",
    "\n",
    "Now, since digital screens are most often displaying images for humans, it makes sense to optimize them to stimulate human eyes. That's why screen are a vast array of very little light sources producing red, green and blue light, often refered to as an *RGB pixel array*. This means that every single pixel needs 3 values to properly display its color.\n",
    "\n",
    "Run the following code to check this fact out. Up close, you clearly see alternating red and blue squares, similar to pixels in which the red and blue light would be on. By standing several meters away from your screen (or by reducing `square_size`), the squares vanish and you only see purple.\n",
    "\n",
    "You can also adapt the code to test other combinations of colors for yourself! Green and red is an interesting one. What do you expect as color? What do you actually see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19a2fbe679764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dimensions\n",
    "width, height = 1980, 1080\n",
    "square_size = 30\n",
    "\n",
    "# Define colors using RGB standards\n",
    "red = [255, 0, 0]\n",
    "green = [0, 255, 0]\n",
    "blue = [0, 0, 255]\n",
    "\n",
    "# Empty image\n",
    "color_array = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "# Loop over rows and columns\n",
    "for row in range(0, height, square_size):\n",
    "    for column in range(0, width, square_size):\n",
    "        # alternate color between red and blue\n",
    "        if (column + row) // square_size % 2 == 0:\n",
    "            color = red\n",
    "        else:\n",
    "            color = blue\n",
    "        # fill the color in the square\n",
    "        color_array[row:row+square_size, column:column+square_size] = color\n",
    "\n",
    "# Display the image\n",
    "plt.figure()\n",
    "plt.imshow(color_array)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc6e7d4a486227",
   "metadata": {},
   "source": [
    "There is still one detail to discuss: where does the number 255 come from? Is this a random number or does it correspond to something? What happens if we plug in a smaller number than 255, or a larger one?\n",
    "\n",
    "*Hint: If you plug in a larger number than 255, Numpy will help you figure it out with a nice red text!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6081193575a58050",
   "metadata": {},
   "source": [
    "## Color detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdce381c5ae7c503",
   "metadata": {},
   "source": [
    "Now we know everything to start doing interesting stuff! Let's try to detect red, green and blue in an image.\n",
    "\n",
    "Start a new video capture using your webcam and capture one frame of some colored object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67940a45ac4611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start video capture\n",
    "cap = ...\n",
    "# since the webcam needs some time to adjust to the light, it's best to wait for a fraction of a second\n",
    "_ = cv2.waitKey(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee9ba39f607ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a frame with a colored object\n",
    "\n",
    "\n",
    "# Display the image\n",
    "\n",
    "\n",
    "# release your video capture!\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb78043b3e14c2ae",
   "metadata": {},
   "source": [
    "We already know how to define red, green and blue. Could you define the color yellow and black using RGB standards?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba4ed106b7bb511",
   "metadata": {},
   "outputs": [],
   "source": [
    "red = [255, 0, 0]\n",
    "green = [0, 255, 0]\n",
    "blue = [0, 0, 255]\n",
    "\n",
    "yellow = []\n",
    "black = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd15670bdde01c5",
   "metadata": {},
   "source": [
    "Ok let's try to find the colors in our frame. The most obvious way to proceed is to check every pixel in our array and check if its exact value corresponds to one of the colors that we defined. We will first try with red.\n",
    "\n",
    "Run the following code to find the amount of red pixels in the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533588a8119a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_red_pixels = 0\n",
    "n_pixels = 0\n",
    "for pixel in frame.flatten():\n",
    "    n_pixels += 1\n",
    "    if np.array_equal(pixel, red):\n",
    "        n_red_pixels += 1\n",
    "\n",
    "print(f\"There are {n_pixels} in the frame, from which {n_red_pixels} are red pixels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded194496f0785a4",
   "metadata": {},
   "source": [
    "This was not only slow, but you probably did not find a single red pixel in the frame. This is normal!\n",
    "\n",
    "In reality, all colors around you give combinations of red, green, and blue when picked up by a camera. The best example is light and dark shades, human could call them light and dark red, whereas computers may refer to them as [255, 70, 70] and [150, 20, 20], respectively. Every pixel in the frame will have some combination of **[R, G, B]**, so finding out the color of a pixel is always a 3 number problem.\n",
    "\n",
    "The easiest way to proceed is to transform our RGB color space into one that is more adapted to color detection for computers: **the HSV color space**!\n",
    "HSV stands for Hue, Saturation, and Value. And very simply put, the hue maps the actual colors of the rainbow to a range of numbers from 0 to 180, whereas the saturation and value together encode what we referred to as shade.\n",
    "Check out the following figure for a visual representation of the RGB and the HSV color spaces.\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/8/83/RGB_Cube_Show_lowgamma_cutout_b.png\" style=\"width:47%;\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/3/33/HSV_color_solid_cylinder_saturation_gray.png\" style=\"width:47%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50366f14e6a784df",
   "metadata": {},
   "source": [
    "So, to conclude:\n",
    "\n",
    "A video is a series of frames shown very rapidly after each other.\n",
    "A frame is a collection of pixels usually represented by an amount of red, green and blue.\n",
    "To identify colors, we need to transform our frame so that the color is represented by one single number: the hue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749d3a315df60df3",
   "metadata": {},
   "source": [
    "## Color detection in Python using OpenCV\n",
    "\n",
    "Enough theory, now we know everything needed to start color detection!\n",
    "\n",
    "Run the following cell to transform your image from the RGB color space to the HSV color space.\n",
    "\n",
    "*Note: You may see that the conversion is actually done from BGR to HSV. The reason is simply that OpenCV stores the blue first and the red last, hence BGR. Does that ring a bell to one of the weird lines above?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25eecbfa8936694",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87260baa1018c5",
   "metadata": {},
   "source": [
    "You will probably need to take a lot of new captures from you webcam to check the color detection on different objects. Here is a very small function that you can call to get a frame in BGR and HSV color space from your capture.\n",
    "\n",
    "*Note: Be careful, the video capture does not get released*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb65923394144",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def get_frame(cap):\n",
    "    if not cap.isOpened():\n",
    "        cap = cv2.VideoCapture(0)\n",
    "\n",
    "    _ = cv2.waitKey(100)\n",
    "\n",
    "    _, bgr_frame = cap.read()\n",
    "    hsv_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2HSV)\n",
    "    return bgr_frame, hsv_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba81af4b4deb1e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now use this line of code to get a new frame\n",
    "frame, hsv_frame = get_frame(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b49dc583d9b1e",
   "metadata": {},
   "source": [
    "Let's start with blue.\n",
    "\n",
    "In the HSV color space, blueish colors are somewhat in the middle of the range of hues, let's say from 100 to 125. The saturation and the value range from 0 to 255, and since we already defined the exact color by its hue, we can set a broad range for both these numbers to find as many shades of blue as possible. Just remember that a saturation close to 0 corresponds to very gray shades and a value close to 0 corresponds to very dark shades. You can check the figure above for visual confirmation. The output of `cv2.inRange()` is a mask, this is an array of the same shape as the input with 0 where nothing was detected and 255 where something was detected.\n",
    "\n",
    "Run the following cell to identify blue regions in the frame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1801f1f73798829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower bound for blue\n",
    "low_blue = np.array([100, 50, 50])\n",
    "# upper bound for blue\n",
    "high_blue = np.array([125, 255, 255])\n",
    "# find any pixel that sits between low blue and high blue\n",
    "blue_mask = cv2.inRange(hsv_frame, low_blue, high_blue)\n",
    "# draw mask, this is the function that we defined at the start of this notebook\n",
    "blue_frame = draw_contours(frame, blue_mask, blue)\n",
    "\n",
    "# plot\n",
    "plt.figure()\n",
    "rgb_blue_frame = cv2.cvtColor(blue_frame, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(rgb_blue_frame)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb6e5650bd122",
   "metadata": {},
   "source": [
    "And let's do the same for green.\n",
    "\n",
    "In the HSV color space, greenish colors correspond are somewhat at the center of the range of hues, let's say from 40 to 90. Don't forget to use a broad range of saturation and value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe2e8c94c78014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# range\n",
    "low_green = np.array([])\n",
    "high_green = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c95d310090014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.any(low_green), \"Low green is empty\"\n",
    "assert np.any(high_green), \"High green is empty\"\n",
    "\n",
    "# mask\n",
    "green_mask = cv2.inRange(hsv_frame, low_green, high_green)\n",
    "# draw mask\n",
    "green_frame = draw_contours(frame, green_mask, green)\n",
    "\n",
    "# plot\n",
    "plt.figure()\n",
    "rgb_green_frame = cv2.cvtColor(green_frame, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(rgb_green_frame)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed1b8266132bc2",
   "metadata": {},
   "source": [
    "For red, there is one extra step to take. In the HSV color space of OpenCV, reddish colors are defined both from 0 to 20 and from 160 to 180. So we need to check two ranges. Luckily, both inputs and outputs of `cv2.inRange()` are numpy arrays, so you can simply add the red masks together!\n",
    "\n",
    "Another complication with red is that white color skin tends to overlap with reddish colors in HSV color space. To avoid these pale reddish colors, use a stricter range on the saturation to get only vibrant reds!\n",
    "\n",
    "Fill the following cell to detect red!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff6e5479d9a4604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# range and mask 1\n",
    "\n",
    "# range and mask 2\n",
    "\n",
    "# add masks together to detect red color\n",
    "\n",
    "# draw mask\n",
    "\n",
    "\n",
    "# plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233cecd7882279f",
   "metadata": {},
   "source": [
    "As a last exercice, can you detect black using this method? Or what about white or gray? Can you use the exact same logic for these colors or not? Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459f372bf29b1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe9d6bf281c1af9b",
   "metadata": {},
   "source": [
    "Please release any video capture before going to the final part of this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d161611b3afd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdef4a5e1c916555",
   "metadata": {},
   "source": [
    "## Color detection in a real-time video\n",
    "\n",
    "Let's put everything together and detect all the colors in real time using your webcam! You will need to find the hue corresponding to yellow yourself. You can use the internet for that, or just maybe we can find it using a couple of lines of code in OpenCV?\n",
    "\n",
    "Complete and run the following cells!\n",
    "\n",
    "*Note: Make sure to draw the contours on the same frame to see all the detected colors at once!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57733494d590746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video capture setup\n",
    "cap = ...\n",
    "\n",
    "# infinite loop\n",
    "while True:\n",
    "\n",
    "    # get a frame\n",
    "    ok, frame = ...\n",
    "\n",
    "    # get HSV frame\n",
    "    hsv_frame = ...\n",
    "\n",
    "    # blue\n",
    "\n",
    "\n",
    "    # green\n",
    "\n",
    "\n",
    "    # red\n",
    "\n",
    "\n",
    "    # yellow\n",
    "\n",
    "\n",
    "    # show frame\n",
    "\n",
    "\n",
    "    # to quit\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# release the capture and close windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da6fa9b4a92b7eb",
   "metadata": {},
   "source": [
    "## Use color as an input to our system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc11d43ebd59b2",
   "metadata": {},
   "source": [
    "Let's try to follow an object of a specific color.\n",
    "Complete and run the following cells to try the color based object tracking!\n",
    "\n",
    "The tighter the range can be around the colored object, the better it will work! Modify `low` and `high` to best fit the color of the object you want to track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7eb48e75bb4b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color of object to track in HSV color space\n",
    "low = np.array([100, 50, 50])\n",
    "high = np.array([125, 255, 255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b7361-8a93-4a76-a48c-1b68ebc8756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video capture setup\n",
    "cap = ...\n",
    "\n",
    "# Define a trail mask with the same size as our frame to store the trail of the object\n",
    "width  = ...\n",
    "height = ...\n",
    "trail_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "last_COM = (0,0)\n",
    "\n",
    "# Infinite loop\n",
    "while True:\n",
    "\n",
    "    # get a frame\n",
    "    _, frame = ...\n",
    "\n",
    "    # get HSV frame\n",
    "    hsv_frame = ...\n",
    "\n",
    "    # get mask of the color\n",
    "    mask = ...\n",
    "\n",
    "\n",
    "    # Let's now do something special with the colored object, like drawing a trace follow it.\n",
    "    # You can try understand this piece of code, or just accept what it does. Up to you!\n",
    "    # ----------------------------------------------------------------------------------------- #\n",
    "\n",
    "    # Find contours in the masked image, these are the outlines of connected regions in the image\n",
    "    cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If we find any contours\n",
    "    if len(cnts) > 0:\n",
    "        # Sort the contours using area\n",
    "        cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "        # Find the largest contour\n",
    "        cnt = cnts[0]\n",
    "\n",
    "        # Calculate center of mass of contoured area\n",
    "        M = cv2.moments(cnt)\n",
    "        \n",
    "        # If a mass is found, we draw a trail\n",
    "        if M[\"m00\"] > 0:\n",
    "\n",
    "            # Computer center of mass\n",
    "            COM = (int(M['m10'] / M['m00']), int(M['m01'] / M['m00']))\n",
    "                \n",
    "            # We first check that the distance between the COMs is not too large\n",
    "            if np.linalg.norm([last_COM[0] - COM[0], last_COM[1] - COM[1]]) < height/15:\n",
    "                color = frame[COM[1], COM[0]].tolist()\n",
    "                cv2.line(trail_mask, COM, last_COM, color, 3)\n",
    "                \n",
    "            # Set last COM for next loop\n",
    "            last_COM = COM\n",
    "\n",
    "    # Add the trail onto the most recent frame\n",
    "    frame = cv2.addWeighted(frame, 1, trail_mask, 1, 0)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------- #\n",
    "\n",
    "    # Show image\n",
    "    ...\n",
    "\n",
    "    # To quit, press escape\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release capture\n",
    "...\n",
    "# Destroy all windows\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
